{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Principal Component Analysis (PCA)\n",
    "We can implement PCA using pure numpy. Before we get started, let's import some necessary packages.  We will need numpy and numpy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need some data.  To begin with, we'll just use some arbitrary, made-up data.  Note, though, that the columns have to be linearly independent or we could end up with a singular matrix.  Think of it like this, if we do have data with linearly dependent columns, then that means that the column data only differs by a scalar factor.  Because of this, that column contributes no information above what is contributed by the other column and, so, we can discard it.  When creating data, it's easy to create linearly dependent columns.  In the real world, though, it's rare to have to worry about this.  So, let's start with this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [24, 36, 15],\n",
    "    [44, 25, 63],\n",
    "    [37, 84, 29],\n",
    "    [11, 31, 26]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is a 4 x 3 matrix, so we have N (number of samples) is 4 and M (dimensions or number of features) is 3.  The columns are linearly independent, so we are all set!\n",
    "\n",
    "Since we have 3 dimensions, it's not out of the question that we could plot this data.  But, for our purposes, let's suppose that we want to visualize this data in two dimensions instead.  So we are going to set our dimensions in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our plan is to project our three dimensional data into a two dimensional subspace such that the variance is maximized along the two principal components which span the subspace.  The first thing we need is a covariance matrix that represents our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = np.cov(X, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use the `rowvar=False` flag.  By default, `numpy.cov` expects your features to be in rows and data points in colums.  This seems odd, so it's important to remember this about `numpy.cov`.\n",
    "\n",
    "Given our covariance matrix $\\mathbf{C}$, we now compute the eigendecomposition of $\\mathbf{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w,v = la.eig(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.linalg.eig` computes the eigendecomposition of a matrix and returns the eigenvalues and eigenvectors respectively.  For PCA, we want to reorder the eigenvalues from highest to lowest and then sort the eigenvectors to match the sorted eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = w.argsort()[::-1]\n",
    "W = v[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to project to our lower-dimensional sub-space.  In this case, we are projecting from three dimensions to `d = 2` dimensions.  Remember, before projecting, we have to mean center our data by subtracting the mean of each feature column from each datapoint.  The equation for projecting is:\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{W}^T(\\mathbf{x} - \\mathbf{m})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.26039012 -18.99573917]\n",
      " [ 28.88809528  25.2195718 ]\n",
      " [-38.58708635  13.84373353]\n",
      " [  9.43860095 -20.06756616]]\n"
     ]
    }
   ],
   "source": [
    "Z = (X - np.mean(X, axis=0)).dot(W[:, 0:d])\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to scikit learn's PCA implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.26039012 -18.99573917]\n",
      " [-28.88809528  25.2195718 ]\n",
      " [ 38.58708635  13.84373353]\n",
      " [ -9.43860095 -20.06756616]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=d)\n",
    "Z_pca = pca.fit_transform(X)\n",
    "print(Z_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute values match up, but the data seems to be a reflection of itself.  I'm not sure why this is, but PCA is, indeed, working in both cases and they're equivalent.\n",
    "For completeness, the code above is codified in a python class below and shown to give the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mypca:\n",
      "[[  0.26039012 -18.99573917]\n",
      " [ 28.88809528  25.2195718 ]\n",
      " [-38.58708635  13.84373353]\n",
      " [  9.43860095 -20.06756616]]\n",
      "pca explained_variance: [ 603.16006817  397.80526964]\n",
      "pca explained variance ratio: [ 0.58548572  0.38614841]\n",
      "mypca explained_variance: [ 804.21342423  530.40702619]\n",
      "mypca explained variance ratio: [ 0.58548572  0.38614841]\n"
     ]
    }
   ],
   "source": [
    "class MyPCA:\n",
    "\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # C is the covariance matrix of X\n",
    "        C = np.cov(X, rowvar=False)\n",
    "\n",
    "        # Perform an eigendecomposition of C to obtain the eigenvalues\n",
    "        # and eigenvectors. w contains the eigenvalues and v contains\n",
    "        # the eigenvectors\n",
    "        w, v = la.eig(C)\n",
    "\n",
    "        # Sort the eigenvalues highest to lowest and store the ordered\n",
    "        # eigenvectors in W\n",
    "        i = w.argsort()[::-1]\n",
    "        W = v[:, i]\n",
    "\n",
    "        # Store the proportion of variance explained stats\n",
    "        ev = np.sort(w)[::-1][0:self.n_components]\n",
    "        evr = ev / np.sum(w)\n",
    "        self.explained_variance_ = ev\n",
    "        self.explained_variance_ratio_ = evr\n",
    "        self.components_ = W[:, 0:self.n_components].T\n",
    "\n",
    "        # Before projecting, mean center our data by subtracting the\n",
    "        # mean by columns (axis=0).  Then project onto the subspace\n",
    "        # represented by the n_components eigenvectors\n",
    "        return (X - np.mean(X, axis=0)).dot(W[:, 0:self.n_components])\n",
    "\n",
    "mypca = MyPCA(n_components=d)\n",
    "X_mypca = mypca.fit_transform(X)\n",
    "\n",
    "print('X_mypca:\\n{}'.format(X_mypca))\n",
    "\n",
    "print('pca explained_variance: {}'.format(pca.explained_variance_))\n",
    "print('pca explained variance ratio: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "print('mypca explained_variance: {}'.format(mypca.explained_variance_))\n",
    "print('mypca explained variance ratio: {}'.format(mypca.explained_variance_ratio_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
